{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2q27gKz1H20"
      },
      "source": [
        "##### Copyright 2023 The MediaPipe Authors. All Rights Reserved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "TUfAcER1oUS6"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_cQX8dWu4Dv"
      },
      "source": [
        "# Face Detection with MediaPipe Tasks\n",
        "\n",
        "This notebook shows you how to use the MediaPipe Tasks Python API to detect faces in images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6PN9FvIx614"
      },
      "source": [
        "## Preparation\n",
        "\n",
        "Let's start with installing MediaPipe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "gxbHBsF-8Y_l"
      },
      "outputs": [],
      "source": [
        "#!pip install -q mediapipe==0.10.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a49D7h4TVmru"
      },
      "source": [
        "Then download an off-the-shelf model. Check out the [MediaPipe documentation](https://developers.google.com/mediapipe/solutions/vision/face_detector#models) for more face detection models that you can use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OMjuVQiDYJKF"
      },
      "outputs": [],
      "source": [
        "#!wget -q -O detector.tflite -q https://storage.googleapis.com/mediapipe-models/face_detector/blaze_face_short_range/float16/1/blaze_face_short_range.tflite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89BlskiiyGDC"
      },
      "source": [
        "## Visualization utilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLHhoIkkWYLQ"
      },
      "source": [
        "To better demonstrate the Face Detector API, we have created a set of visualization tools that will be used in this colab. These will draw a bounding box around detected faces, as well as markers over certain detected points on the faces."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "H4aPO-hvbw3r"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple, Union\n",
        "import math\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "MARGIN = 10  # pixels\n",
        "ROW_SIZE = 10  # pixels\n",
        "FONT_SIZE = 1\n",
        "FONT_THICKNESS = 1\n",
        "TEXT_COLOR = (255, 0, 0)  # red\n",
        "\n",
        "\n",
        "def _normalized_to_pixel_coordinates(\n",
        "    normalized_x: float, normalized_y: float, image_width: int,\n",
        "    image_height: int) -> Union[None, Tuple[int, int]]:\n",
        "  \"\"\"Converts normalized value pair to pixel coordinates.\"\"\"\n",
        "\n",
        "  # Checks if the float value is between 0 and 1.\n",
        "  def is_valid_normalized_value(value: float) -> bool:\n",
        "    return (value > 0 or math.isclose(0, value)) and (value < 1 or\n",
        "                                                      math.isclose(1, value))\n",
        "\n",
        "  if not (is_valid_normalized_value(normalized_x) and\n",
        "          is_valid_normalized_value(normalized_y)):\n",
        "    # TODO: Draw coordinates even if it's outside of the image bounds.\n",
        "    return None\n",
        "  x_px = min(math.floor(normalized_x * image_width), image_width - 1)\n",
        "  y_px = min(math.floor(normalized_y * image_height), image_height - 1)\n",
        "  return x_px, y_px\n",
        "\n",
        "\n",
        "def visualize(\n",
        "    image,\n",
        "    detection_result\n",
        ") -> np.ndarray:\n",
        "  \"\"\"Draws bounding boxes and keypoints on the input image and return it.\n",
        "  Args:\n",
        "    image: The input RGB image.\n",
        "    detection_result: The list of all \"Detection\" entities to be visualize.\n",
        "  Returns:\n",
        "    Image with bounding boxes.\n",
        "  \"\"\"\n",
        "  annotated_image = image.copy()\n",
        "  height, width, _ = image.shape\n",
        "\n",
        "  for detection in detection_result.detections:\n",
        "    # Draw bounding_box\n",
        "    bbox = detection.bounding_box\n",
        "    start_point = bbox.origin_x, bbox.origin_y\n",
        "    end_point = bbox.origin_x + bbox.width, bbox.origin_y + bbox.height\n",
        "    cv2.rectangle(annotated_image, start_point, end_point, TEXT_COLOR, 3)\n",
        "\n",
        "    # Draw keypoints\n",
        "    for keypoint in detection.keypoints:\n",
        "      keypoint_px = _normalized_to_pixel_coordinates(keypoint.x, keypoint.y,\n",
        "                                                     width, height)\n",
        "      color, thickness, radius = (0, 255, 0), 2, 2\n",
        "      cv2.circle(annotated_image, keypoint_px, thickness, color, radius)\n",
        "\n",
        "    # Draw label and score\n",
        "    category = detection.categories[0]\n",
        "    category_name = category.category_name\n",
        "    category_name = '' if category_name is None else category_name\n",
        "    probability = round(category.score, 2)\n",
        "    result_text = category_name + ' (' + str(probability) + ')'\n",
        "    text_location = (MARGIN + bbox.origin_x,\n",
        "                     MARGIN + ROW_SIZE + bbox.origin_y)\n",
        "    cv2.putText(annotated_image, result_text, text_location, cv2.FONT_HERSHEY_PLAIN,\n",
        "                FONT_SIZE, TEXT_COLOR, FONT_THICKNESS)\n",
        "\n",
        "  return annotated_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83PEJNp9yPBU"
      },
      "source": [
        "## Download test image\n",
        "\n",
        "To demonstrate Face Detection, you can download a sample image using the following code. Credits: https://pixabay.com/photos/brother-sister-girl-family-boy-977170/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tzXuqyIBlXer"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[ WARN:0@9.065] global loadsave.cpp:248 findDecoder imread_('photos/brendan-guillermo-michelle-225236-1.jpg'): can't open/read file: check file path/integrity\n"
          ]
        },
        {
          "ename": "error",
          "evalue": "OpenCV(4.8.0) /Users/xperience/GHA-OpenCV-Python/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39m#from google.colab.patches import cv2_imshow\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39m#img = cv2.imread(IMAGE_FILE)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39m#cv2.imshow(img)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m img \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mimread(IMAGE_FILE)\n\u001b[0;32m---> 13\u001b[0m cv2\u001b[39m.\u001b[39;49mcvtColor(img, cv2\u001b[39m.\u001b[39;49mCOLOR_BGR2RGB) \u001b[39m# Converting BGR to RGB\u001b[39;00m\n\u001b[1;32m     14\u001b[0m display(Image\u001b[39m.\u001b[39mfromarray(img))\n",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.8.0) /Users/xperience/GHA-OpenCV-Python/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
          ]
        }
      ],
      "source": [
        "#!wget -q -O image.jpg https://i.imgur.com/Vu2Nqwb.jpg\n",
        "\n",
        "IMAGE_FILE = 'photos/brendan-guillermo-michelle-225236-1.jpg'\n",
        "\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "\n",
        "#from google.colab.patches import cv2_imshow\n",
        "#img = cv2.imread(IMAGE_FILE)\n",
        "#cv2.imshow(img)\n",
        "img = cv2.imread(IMAGE_FILE)\n",
        "cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # Converting BGR to RGB\n",
        "display(Image.fromarray(img))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAFQm3HHi5OG"
      },
      "source": [
        "Optionally, you can upload your own image from your computer. To do this, uncomment the following code cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gwip05yi6lV"
      },
      "outputs": [],
      "source": [
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "\n",
        "# for filename in uploaded:\n",
        "#   content = uploaded[filename]\n",
        "#   with open(filename, 'wb') as f:\n",
        "#     f.write(content)\n",
        "\n",
        "# if len(uploaded.keys()):\n",
        "#   IMAGE_FILE = next(iter(uploaded))\n",
        "#   print('Uploaded file:', IMAGE_FILE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iy4r2_ePylIa"
      },
      "source": [
        "## Running inference and visualizing the results\n",
        "\n",
        "The final step is to run face detection on your selected image. This involves creating your FaceDetector object, loading your image, running detection, and finally, the optional step of displaying the image with visualizations.\n",
        "\n",
        "You can check out the [MediaPipe documentation](https://developers.google.com/mediapipe/solutions/vision/face_detector/python) to learn more about configuration options that this solution supports."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Yl_Oiye4mUuo"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Unable to open file at /Users/vsanz/Code/projects/face_recognition_and_redis_vss/models/blaze_face_short_range.tflite",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m base_options \u001b[39m=\u001b[39m python\u001b[39m.\u001b[39mBaseOptions(model_asset_path\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmodels/blaze_face_short_range.tflite\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m options \u001b[39m=\u001b[39m vision\u001b[39m.\u001b[39mFaceDetectorOptions(base_options\u001b[39m=\u001b[39mbase_options)\n\u001b[0;32m---> 10\u001b[0m detector \u001b[39m=\u001b[39m vision\u001b[39m.\u001b[39;49mFaceDetector\u001b[39m.\u001b[39;49mcreate_from_options(options)\n\u001b[1;32m     12\u001b[0m \u001b[39m# STEP 3: Load the input image.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m image \u001b[39m=\u001b[39m mp\u001b[39m.\u001b[39mImage\u001b[39m.\u001b[39mcreate_from_file(IMAGE_FILE)\n",
            "File \u001b[0;32m~/Code/projects/face_recognition_and_redis_vss/.env/lib/python3.11/site-packages/mediapipe/tasks/python/vision/face_detector.py:181\u001b[0m, in \u001b[0;36mFaceDetector.create_from_options\u001b[0;34m(cls, options)\u001b[0m\n\u001b[1;32m    163\u001b[0m   options\u001b[39m.\u001b[39mresult_callback(\n\u001b[1;32m    164\u001b[0m       detection_result,\n\u001b[1;32m    165\u001b[0m       image,\n\u001b[1;32m    166\u001b[0m       timestamp\u001b[39m.\u001b[39mvalue \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m _MICRO_SECONDS_PER_MILLISECOND,\n\u001b[1;32m    167\u001b[0m   )\n\u001b[1;32m    169\u001b[0m task_info \u001b[39m=\u001b[39m _TaskInfo(\n\u001b[1;32m    170\u001b[0m     task_graph\u001b[39m=\u001b[39m_TASK_GRAPH_NAME,\n\u001b[1;32m    171\u001b[0m     input_streams\u001b[39m=\u001b[39m[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    179\u001b[0m     task_options\u001b[39m=\u001b[39moptions,\n\u001b[1;32m    180\u001b[0m )\n\u001b[0;32m--> 181\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m(\n\u001b[1;32m    182\u001b[0m     task_info\u001b[39m.\u001b[39;49mgenerate_graph_config(\n\u001b[1;32m    183\u001b[0m         enable_flow_limiting\u001b[39m=\u001b[39;49moptions\u001b[39m.\u001b[39;49mrunning_mode\n\u001b[1;32m    184\u001b[0m         \u001b[39m==\u001b[39;49m _RunningMode\u001b[39m.\u001b[39;49mLIVE_STREAM\n\u001b[1;32m    185\u001b[0m     ),\n\u001b[1;32m    186\u001b[0m     options\u001b[39m.\u001b[39;49mrunning_mode,\n\u001b[1;32m    187\u001b[0m     packets_callback \u001b[39mif\u001b[39;49;00m options\u001b[39m.\u001b[39;49mresult_callback \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    188\u001b[0m )\n",
            "File \u001b[0;32m~/Code/projects/face_recognition_and_redis_vss/.env/lib/python3.11/site-packages/mediapipe/tasks/python/vision/core/base_vision_task_api.py:70\u001b[0m, in \u001b[0;36mBaseVisionTaskApi.__init__\u001b[0;34m(self, graph_config, running_mode, packet_callback)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39melif\u001b[39;00m packet_callback:\n\u001b[1;32m     66\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     67\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mThe vision task is in image or video mode, a user-defined result \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     68\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mcallback should not be provided.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     69\u001b[0m   )\n\u001b[0;32m---> 70\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_runner \u001b[39m=\u001b[39m _TaskRunner\u001b[39m.\u001b[39;49mcreate(graph_config, packet_callback)\n\u001b[1;32m     71\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_running_mode \u001b[39m=\u001b[39m running_mode\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Unable to open file at /Users/vsanz/Code/projects/face_recognition_and_redis_vss/models/blaze_face_short_range.tflite"
          ]
        }
      ],
      "source": [
        "# STEP 1: Import the necessary modules.\n",
        "import numpy as np\n",
        "import mediapipe as mp\n",
        "from mediapipe.tasks import python\n",
        "from mediapipe.tasks.python import vision\n",
        "\n",
        "# STEP 2: Create an FaceDetector object.\n",
        "base_options = python.BaseOptions(model_asset_path='models/blaze_face_short_range.tflite')\n",
        "options = vision.FaceDetectorOptions(base_options=base_options)\n",
        "detector = vision.FaceDetector.create_from_options(options)\n",
        "\n",
        "# STEP 3: Load the input image.\n",
        "image = mp.Image.create_from_file(IMAGE_FILE)\n",
        "\n",
        "# STEP 4: Detect faces in the input image.\n",
        "detection_result = detector.detect(image)\n",
        "\n",
        "# STEP 5: Process the detection result. In this case, visualize it.\n",
        "image_copy = np.copy(image.numpy_view())\n",
        "annotated_image = visualize(image_copy, detection_result)\n",
        "rgb_annotated_image = cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB)\n",
        "display(Image.fromarray(rgb_annotated_image))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNJq-ygtZX7J"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Unable to open file at /Users/vsanz/Code/projects/face_recognition_and_redis_vss/models/blaze_face_short_range.tflite",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m base_options \u001b[39m=\u001b[39m python\u001b[39m.\u001b[39mBaseOptions(model_asset_path\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmodels/blaze_face_short_range.tflite\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m options \u001b[39m=\u001b[39m vision\u001b[39m.\u001b[39mFaceDetectorOptions(base_options\u001b[39m=\u001b[39mbase_options)\n\u001b[0;32m---> 13\u001b[0m \u001b[39mwith\u001b[39;00m vision\u001b[39m.\u001b[39;49mFaceDetector\u001b[39m.\u001b[39;49mcreate_from_options(options) \u001b[39mas\u001b[39;00m detector:\n\u001b[1;32m     14\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     15\u001b[0m         \u001b[39m# Capture frame-by-frame\u001b[39;00m\n\u001b[1;32m     16\u001b[0m         ret, frame \u001b[39m=\u001b[39m video_capture\u001b[39m.\u001b[39mread()\n",
            "File \u001b[0;32m~/Code/projects/face_recognition_and_redis_vss/.env/lib/python3.11/site-packages/mediapipe/tasks/python/vision/face_detector.py:181\u001b[0m, in \u001b[0;36mFaceDetector.create_from_options\u001b[0;34m(cls, options)\u001b[0m\n\u001b[1;32m    163\u001b[0m   options\u001b[39m.\u001b[39mresult_callback(\n\u001b[1;32m    164\u001b[0m       detection_result,\n\u001b[1;32m    165\u001b[0m       image,\n\u001b[1;32m    166\u001b[0m       timestamp\u001b[39m.\u001b[39mvalue \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m _MICRO_SECONDS_PER_MILLISECOND,\n\u001b[1;32m    167\u001b[0m   )\n\u001b[1;32m    169\u001b[0m task_info \u001b[39m=\u001b[39m _TaskInfo(\n\u001b[1;32m    170\u001b[0m     task_graph\u001b[39m=\u001b[39m_TASK_GRAPH_NAME,\n\u001b[1;32m    171\u001b[0m     input_streams\u001b[39m=\u001b[39m[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    179\u001b[0m     task_options\u001b[39m=\u001b[39moptions,\n\u001b[1;32m    180\u001b[0m )\n\u001b[0;32m--> 181\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m(\n\u001b[1;32m    182\u001b[0m     task_info\u001b[39m.\u001b[39;49mgenerate_graph_config(\n\u001b[1;32m    183\u001b[0m         enable_flow_limiting\u001b[39m=\u001b[39;49moptions\u001b[39m.\u001b[39;49mrunning_mode\n\u001b[1;32m    184\u001b[0m         \u001b[39m==\u001b[39;49m _RunningMode\u001b[39m.\u001b[39;49mLIVE_STREAM\n\u001b[1;32m    185\u001b[0m     ),\n\u001b[1;32m    186\u001b[0m     options\u001b[39m.\u001b[39;49mrunning_mode,\n\u001b[1;32m    187\u001b[0m     packets_callback \u001b[39mif\u001b[39;49;00m options\u001b[39m.\u001b[39;49mresult_callback \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    188\u001b[0m )\n",
            "File \u001b[0;32m~/Code/projects/face_recognition_and_redis_vss/.env/lib/python3.11/site-packages/mediapipe/tasks/python/vision/core/base_vision_task_api.py:70\u001b[0m, in \u001b[0;36mBaseVisionTaskApi.__init__\u001b[0;34m(self, graph_config, running_mode, packet_callback)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39melif\u001b[39;00m packet_callback:\n\u001b[1;32m     66\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     67\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mThe vision task is in image or video mode, a user-defined result \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     68\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mcallback should not be provided.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     69\u001b[0m   )\n\u001b[0;32m---> 70\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_runner \u001b[39m=\u001b[39m _TaskRunner\u001b[39m.\u001b[39;49mcreate(graph_config, packet_callback)\n\u001b[1;32m     71\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_running_mode \u001b[39m=\u001b[39m running_mode\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Unable to open file at /Users/vsanz/Code/projects/face_recognition_and_redis_vss/models/blaze_face_short_range.tflite"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import sys\n",
        "import numpy as np\n",
        "import mediapipe as mp\n",
        "from mediapipe.tasks import python\n",
        "from mediapipe.tasks.python import vision\n",
        "\n",
        "video_capture = cv2.VideoCapture(0)\n",
        "\n",
        "# STEP 2: Create an FaceDetector object.\n",
        "base_options = python.BaseOptions(model_asset_path='models/blaze_face_short_range.tflite')\n",
        "options = vision.FaceDetectorOptions(base_options=base_options)\n",
        "with vision.FaceDetector.create_from_options(options) as detector:\n",
        "    while True:\n",
        "        # Capture frame-by-frame\n",
        "        ret, frame = video_capture.read()\n",
        "        print(ret)\n",
        "        print(frame)\n",
        "\n",
        "        img = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "        detection = detector.detect(frame)\n",
        "\n",
        "        bbox = detection.bounding_box\n",
        "        start_point = bbox.origin_x, bbox.origin_y\n",
        "        end_point = bbox.origin_x + bbox.width, bbox.origin_y + bbox.height\n",
        "        cv2.rectangle(annotated_image, start_point, end_point, TEXT_COLOR, 3)\n",
        "\n",
        "        # Display the resulting frame\n",
        "        cv2.imshow('Video', frame)\n",
        "\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "\n",
        "# When everything is done, release the capture\n",
        "video_capture.release()\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "module 'numpy' has no attribute 'int'.\n`np.int` was a deprecated alias for the builtin `int`. To avoid this error in existing code, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[8], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mif\u001b[39;00m pos \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     27\u001b[0m     yaw, pitch, roll \u001b[39m=\u001b[39m faceOrientation2Euler(ori, degrees\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 28\u001b[0m     face\u001b[39m.\u001b[39;49mdraw_oriented_bounding_box(img, color\u001b[39m=\u001b[39;49mbox_colors[i\u001b[39m%\u001b[39;49m\u001b[39m3\u001b[39;49m], thickness\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     29\u001b[0m     face\u001b[39m.\u001b[39mdraw_reference_frame(img, pos, ori, origin\u001b[39m=\u001b[39mface\u001b[39m.\u001b[39mget_landmark_pos(Face\u001b[39m.\u001b[39mnose_tip_index))\n\u001b[1;32m     31\u001b[0m face\u001b[39m.\u001b[39mdraw_reference_frame(img, pos, ori, origin\u001b[39m=\u001b[39mface\u001b[39m.\u001b[39mget_landmark_pos(Face\u001b[39m.\u001b[39mnose_tip_index))\n",
            "File \u001b[0;32m~/Code/projects/face_recognition_and_redis_vss/.env/lib/python3.11/site-packages/FaceAnalyzer/Face.py:1638\u001b[0m, in \u001b[0;36mFace.draw_oriented_bounding_box\u001b[0;34m(self, image, color, thickness)\u001b[0m\n\u001b[1;32m   1629\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Draws a bounding box around the face that rotates wit the face\u001b[39;00m\n\u001b[1;32m   1630\u001b[0m \n\u001b[1;32m   1631\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1634\u001b[0m \u001b[39m    thickness (int, optional): The line thickness. Defaults to 1.\u001b[39;00m\n\u001b[1;32m   1635\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1636\u001b[0m vertex_ids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_face_outer_vertices()\n\u001b[0;32m-> 1638\u001b[0m pts\u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnpLandmarks[v,:\u001b[39m2\u001b[39;49m]\u001b[39m.\u001b[39;49mastype(np\u001b[39m.\u001b[39;49mint) \u001b[39mfor\u001b[39;49;00m v \u001b[39min\u001b[39;49;00m vertex_ids])\n\u001b[1;32m   1639\u001b[0m \u001b[39mfor\u001b[39;00m pt \u001b[39min\u001b[39;00m pts:\n\u001b[1;32m   1640\u001b[0m     cv2\u001b[39m.\u001b[39mcircle(image, pt, \u001b[39m1\u001b[39m, color, thickness)\n",
            "File \u001b[0;32m~/Code/projects/face_recognition_and_redis_vss/.env/lib/python3.11/site-packages/FaceAnalyzer/Face.py:1638\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1629\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Draws a bounding box around the face that rotates wit the face\u001b[39;00m\n\u001b[1;32m   1630\u001b[0m \n\u001b[1;32m   1631\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1634\u001b[0m \u001b[39m    thickness (int, optional): The line thickness. Defaults to 1.\u001b[39;00m\n\u001b[1;32m   1635\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1636\u001b[0m vertex_ids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_face_outer_vertices()\n\u001b[0;32m-> 1638\u001b[0m pts\u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnpLandmarks[v,:\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39;49mint) \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m vertex_ids])\n\u001b[1;32m   1639\u001b[0m \u001b[39mfor\u001b[39;00m pt \u001b[39min\u001b[39;00m pts:\n\u001b[1;32m   1640\u001b[0m     cv2\u001b[39m.\u001b[39mcircle(image, pt, \u001b[39m1\u001b[39m, color, thickness)\n",
            "File \u001b[0;32m~/Code/projects/face_recognition_and_redis_vss/.env/lib/python3.11/site-packages/numpy/__init__.py:305\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    300\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    301\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIn the future `np.\u001b[39m\u001b[39m{\u001b[39;00mattr\u001b[39m}\u001b[39;00m\u001b[39m` will be defined as the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    302\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcorresponding NumPy scalar.\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFutureWarning\u001b[39;00m, stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m    304\u001b[0m \u001b[39mif\u001b[39;00m attr \u001b[39min\u001b[39;00m __former_attrs__:\n\u001b[0;32m--> 305\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(__former_attrs__[attr])\n\u001b[1;32m    307\u001b[0m \u001b[39m# Importing Tester requires importing all of UnitTest which is not a\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[39m# cheap import Since it is mainly used in test suits, we lazy import it\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[39m# here to save on the order of 10 ms of import time for most users\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \u001b[39m# The previous way Tester was imported also had a side effect of adding\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[39m# the full `numpy.testing` namespace\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[39mif\u001b[39;00m attr \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtesting\u001b[39m\u001b[39m'\u001b[39m:\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'int'.\n`np.int` was a deprecated alias for the builtin `int`. To avoid this error in existing code, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import sys\n",
        "import numpy as np\n",
        "from FaceAnalyzer import FaceAnalyzer, Face\n",
        "from FaceAnalyzer.helpers.geometry.orientation import faceOrientation2Euler\n",
        "\n",
        "video_capture = cv2.VideoCapture(0)\n",
        "box_colors=[\n",
        "    (255,0,0),\n",
        "    (255,0,255),\n",
        "    (255,0,255),\n",
        "]\n",
        "fa = FaceAnalyzer()\n",
        "while True:\n",
        "    # Capture frame-by-frame\n",
        "    ret, frame = video_capture.read()\n",
        "\n",
        "    img = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "    fa.process(img)\n",
        "    if fa.nb_faces>0:\n",
        "        #print(f\"{fa.nb_faces} Faces found\")\n",
        "        \n",
        "        for i in range(fa.nb_faces):\n",
        "            face = fa.faces[i]\n",
        "            pos, ori = face.get_head_posture()\n",
        "            if pos is not None:\n",
        "                yaw, pitch, roll = faceOrientation2Euler(ori, degrees=True)\n",
        "                face.draw_oriented_bounding_box(img, color=box_colors[i%3], thickness=1)\n",
        "                face.draw_reference_frame(img, pos, ori, origin=face.get_landmark_pos(Face.nose_tip_index))\n",
        "\n",
        "            face.draw_reference_frame(img, pos, ori, origin=face.get_landmark_pos(Face.nose_tip_index))\n",
        "            \n",
        "\n",
        "\n",
        "    # bbox = detection.bounding_box\n",
        "    # start_point = bbox.origin_x, bbox.origin_y\n",
        "    # end_point = bbox.origin_x + bbox.width, bbox.origin_y + bbox.height\n",
        "    # cv2.rectangle(annotated_image, start_point, end_point, TEXT_COLOR, 3)\n",
        "\n",
        "    # Display the resulting frame\n",
        "    cv2.imshow('Video', frame)\n",
        "\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "# When everything is done, release the capture\n",
        "video_capture.release()\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
